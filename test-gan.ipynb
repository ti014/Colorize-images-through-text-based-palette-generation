{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# LOAD DATA","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.utils.data as data\nimport pickle\nimport os\nimport numpy as np\nfrom skimage.color import rgb2lab\nimport warnings\n\nclass Test_Dataset(data.Dataset):\n    def __init__(self, input_dict, txt_path, pal_path, img_path, transform=None):\n        self.transform = transform\n        with open(img_path, 'rb') as f:\n            self.images = np.asarray(pickle.load(f)) / 255\n        with open(txt_path, 'rb') as fin:\n            self.src_seqs = pickle.load(fin)\n        with open(pal_path, 'rb') as fin:\n            self.trg_seqs = pickle.load(fin)\n\n        # ==================== Preprocessing src_seqs ====================#\n        # Return a list of indexes, one for each word in the sentence.\n        words_index = []\n        for index, palette_name in enumerate(self.src_seqs):\n            # Set list size to the longest palette name.\n            temp = [0] * input_dict.max_len\n            for i, word in enumerate(palette_name):\n                temp[i] = input_dict.word2index[word]\n            words_index.append(temp)\n\n        self.src_seqs = torch.LongTensor(words_index)\n\n        # ==================== Preprocessing trg_seqs ====================#\n        palette_list = []\n        for palettes in self.trg_seqs:\n            temp = []\n            for palette in palettes:\n                rgb = np.array([palette[0], palette[1], palette[2]]) / 255.0\n                warnings.filterwarnings(\"ignore\")\n                lab = rgb2lab(rgb[np.newaxis, np.newaxis, :], illuminant='D50').flatten()\n                temp.append(lab[0])\n                temp.append(lab[1])\n                temp.append(lab[2])\n            palette_list.append(temp)\n\n        self.trg_seqs = torch.FloatTensor(palette_list)\n\n        self.num_total_data = len(self.src_seqs)\n\n    def __len__(self):\n        return self.num_total_data\n\n    def __getitem__(self, idx):\n        \"\"\"Returns one data pair.\"\"\"\n        text = self.src_seqs[idx]\n        palette = self.trg_seqs[idx]\n        image = self.images[idx]\n        if self.transform:\n            image = self.transform(image)\n\n        return text, palette, image\n\n\ndef test_loader(dataset, batch_size, input_dict):\n\n    if dataset == 'bird256':\n        txt_path = '/kaggle/input/data-text2color/data/hexcolor_vf/all_names.pkl'\n        pal_path = '/kaggle/input/data-text2color/data/hexcolor_vf/test_palettes_rgb.pkl'\n        img_path = '/kaggle/input/data-text2color/data/bird256/test_palette/test_images_origin.txt'\n\n        test_dataset = Test_Dataset(input_dict, txt_path, pal_path, img_path)\n        test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n                                                  batch_size=batch_size,\n                                                  shuffle=False,\n                                                  num_workers=2)\n        imsize = 256\n\n    return test_loader, imsize","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-14T10:02:02.841065Z","iopub.execute_input":"2022-12-14T10:02:02.841517Z","iopub.status.idle":"2022-12-14T10:02:02.860066Z","shell.execute_reply.started":"2022-12-14T10:02:02.841483Z","shell.execute_reply":"2022-12-14T10:02:02.858838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Util","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport warnings\nfrom skimage.color import lab2rgb, rgb2lab\n\n# ======================== For text embeddings ======================== #\nSOS_token = 0\nEOS_token = 1\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nclass Dictionary:\n    def __init__(self):\n        self.word2index = {}\n        self.word2count = {}\n        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n        self.n_words = 2\n        self.max_len = 0\n\n    def index_elements(self, data):\n        for element in data:\n            self.max_len = len(data) if self.max_len < len(data) else self.max_len\n            self.index_element(element)\n\n    def index_element(self, element):\n        if element not in self.word2index:\n            self.word2index[element] = self.n_words\n            self.word2count[element] = 1\n            self.index2word[self.n_words] = element\n            self.n_words += 1\n        else:\n            self.word2count[element] += 1\n\ndef load_pretrained_embedding(dictionary, embed_file, embed_dim):\n    if embed_file is None: return None\n\n    pretrained_embed = {}\n    with open(embed_file, 'r', encoding='utf-8') as f:\n        for line in f:\n            tokens = line.split(' ')\n            word = tokens[0]\n            entries = tokens[1:]\n            if word == '<unk>':\n                continue\n            pretrained_embed[word] = entries\n        f.close()\n\n    vocab_size = len(dictionary) + 2\n    W_emb = np.random.randn(vocab_size, embed_dim).astype('float32')\n    n = 0\n    for word, index in dictionary.items():\n        if word in pretrained_embed:\n            W_emb[index, :] = pretrained_embed[word]\n            n += 1\n\n    print (\"%d/%d vocabs are initialized with GloVe embeddings.\" % (n, vocab_size))\n    return W_emb\n\nclass Embed(nn.Module):\n    def __init__(self, vocab_size, embed_dim, W_emb, train_emb):\n        super(Embed, self).__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n\n        if W_emb is not None:\n            print (\"Using pre-trained word embeddings...\")\n            self.embed.weight = nn.Parameter(W_emb)\n\n        if train_emb == False:\n            print (\"Not training word embeddings...\")\n            self.embed.requires_grad = False\n\n    def forward(self, doc):\n        doc = self.embed(doc)\n        return doc\n\n# ======================== For processing data ======================== #\ndef process_image(image_data, batch_size, imsize):\n    input = torch.zeros(batch_size, 1, imsize, imsize)\n    labels = torch.zeros(batch_size, 2, imsize, imsize)\n    images_np = image_data.numpy().transpose((0, 2, 3, 1))\n\n    for k in range(batch_size):\n        img_lab = rgb2lab(images_np[k], illuminant='D50')\n        img_l = img_lab[:, :, 0] / 100\n        input[k] = torch.from_numpy(np.expand_dims(img_l, 0))\n\n        img_a_scale = (img_lab[:, :, 1:2] + 88) / 185\n        img_b_scale = (img_lab[:, :, 2:3] + 127) / 212\n\n        img_ab_scale = np.concatenate((img_a_scale, img_b_scale), axis=2)\n        labels[k] = torch.from_numpy(img_ab_scale.transpose((2, 0, 1)))\n    return input, labels\n\ndef process_palette_ab(pal_data, batch_size):\n    img_a_scale = (pal_data[:, :, 1:2] + 88) / 185\n    img_b_scale = (pal_data[:, :, 2:3] + 127) / 212\n    img_ab_scale = np.concatenate((img_a_scale, img_b_scale), axis=2)\n    ab_for_global = torch.from_numpy(img_ab_scale).float()\n    ab_for_global = ab_for_global.view(batch_size, 10).unsqueeze(2).unsqueeze(2)\n    return ab_for_global\n\ndef process_palette_lab(pal_data, batch_size):\n    img_l = pal_data[:, :, 0:1] / 100\n    img_a_scale = (pal_data[:, :, 1:2] + 88) / 185\n    img_b_scale = (pal_data[:, :, 2:3] + 127) / 212\n    img_lab_scale = np.concatenate((img_l, img_a_scale, img_b_scale), axis=2)\n    lab_for_global = torch.from_numpy(img_lab_scale).float()\n    lab_for_global = lab_for_global.view(batch_size, 15).unsqueeze(2).unsqueeze(2)\n    return lab_for_global\n\ndef process_global_ab(input_ab, batch_size, always_give_global_hint):\n    X_hist = input_ab\n\n    if always_give_global_hint:\n        B_hist = torch.ones(batch_size, 1, 1, 1)\n    else:\n        B_hist = torch.round(torch.rand(batch_size, 1, 1, 1))\n        for l in range(batch_size):\n            if B_hist[l].numpy() == 0:\n                X_hist[l] = torch.rand(10)\n\n    global_input = torch.cat([X_hist, B_hist], 1)\n    return global_input\n\ndef process_global_lab(input_lab, batch_size, always_give_global_hint):\n    X_hist = input_lab\n\n    if always_give_global_hint:\n        B_hist = torch.ones(batch_size, 1, 1, 1)\n    else:\n        B_hist = torch.round(torch.rand(batch_size, 1, 1, 1))\n        for l in range(batch_size):\n            if B_hist[l].numpy() == 0:\n                X_hist[l] = torch.rand(15)\n\n    global_input = torch.cat([X_hist, B_hist], 1)\n    return global_input\n\ndef process_global_sampling_ab(palette, batch_size, imsize, hist_mean, hist_std):\n    X_hist = palette.to(device)\n    B_hist = torch.ones(batch_size, 1, 1, 1).to(device)\n\n    global_input = torch.cat([X_hist, B_hist], 1)\n    return global_input\n\ndef process_global_sampling_lab(palette, batch_size, imsize, hist_mean, hist_std):\n    X_hist = palette.to(device)\n    B_hist = torch.ones(batch_size, 1, 1, 1).to(device)\n\n    global_input = torch.cat([X_hist, B_hist], 1)\n    return global_input\n\n# ============================= Etc. ============================= #\ndef KL_loss(mu, logvar):\n    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n    KLD = torch.mean(KLD_element).mul_(-0.5)\n    return KLD\n\n\ndef lab2rgb_1d(in_lab, clip=True):\n    warnings.filterwarnings(\"ignore\")\n    tmp_rgb = lab2rgb(in_lab[np.newaxis, np.newaxis, :], illuminant='D50').flatten()\n    if clip:\n        tmp_rgb = np.clip(tmp_rgb, 0, 1)\n    return tmp_rgb\n\ndef init_weights_normal(m):\n    if type(m) == nn.Conv1d:\n        m.weight.data.normal_(0.0, 0.05)\n    if type(m) == nn.Linear:\n        m.weight.data.normal_(0.0, 0.05)","metadata":{"execution":{"iopub.status.busy":"2022-12-14T10:02:02.862027Z","iopub.execute_input":"2022-12-14T10:02:02.863141Z","iopub.status.idle":"2022-12-14T10:02:02.902500Z","shell.execute_reply.started":"2022-12-14T10:02:02.863090Z","shell.execute_reply":"2022-12-14T10:02:02.900929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TPN","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom random import *\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nclass CA_NET(nn.Module):\n\n    def __init__(self):\n        super(CA_NET, self).__init__()\n        self.t_dim = 150\n        self.c_dim = 150\n        self.fc = nn.Linear(self.t_dim, self.c_dim * 2, bias=True)\n        self.relu = nn.ReLU()\n\n    def encode(self, text_embedding):\n        x = self.relu(self.fc(text_embedding))\n        mu = x[:, :, :self.c_dim]\n        logvar = x[:, :, self.c_dim:]\n        return mu, logvar\n\n    def reparametrize(self, mu, logvar):\n        std = logvar.mul(0.5).exp_()\n        # eps = torch.cuda.FloatTensor(std.size()).normal_(0.0, 1)\n        eps = torch.FloatTensor(std.size()).normal_(0.0, 1).to(device)\n        return eps * std + mu\n\n    def forward(self, text_embedding):\n        mu, logvar = self.encode(text_embedding)\n        c_code = self.reparametrize(mu, logvar)\n        return c_code, mu, logvar\n\n\nclass EncoderRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, n_layers, dropout_p, W_emb=None):\n        super(EncoderRNN, self).__init__()\n\n        self.hidden_size = hidden_size\n        self.n_layers = n_layers\n\n        self.embed = Embed(input_size, 300, W_emb, True)\n        self.gru = nn.GRU(300, hidden_size, n_layers, dropout=dropout_p)\n        self.ca_net = CA_NET()\n\n    def forward(self, word_inputs, hidden):\n        embedded = self.embed(word_inputs).transpose(0,1)\n        output, hidden = self.gru(embedded, hidden)\n        c_code, mu, logvar = self.ca_net(output)\n\n        return c_code, hidden, mu, logvar\n\n    def init_hidden(self, batch_size):\n        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n\n        return hidden\n\n\nclass AttnDecoderRNN(nn.Module):\n    def __init__(self, input_dict, hidden_size, n_layers=1, dropout_p=0.1):\n        super(AttnDecoderRNN, self).__init__()\n        self.input_dict = input_dict\n        self.attn = Attn(hidden_size, input_dict.max_len)\n        self.hidden_size = hidden_size\n        self.n_layers = n_layers\n        self.dropout_p = dropout_p\n        self.palette_dim = 3\n\n        self.gru = nn.GRUCell(self.hidden_size + self.palette_dim, hidden_size)\n\n        self.out = nn.Sequential(\n                        nn.Linear(hidden_size, hidden_size),\n                        nn.ReLU(inplace=True),\n                        nn.BatchNorm1d(hidden_size),\n                        nn.Linear(hidden_size,self.palette_dim)\n                   )\n\n    def forward(self, last_palette, last_decoder_hidden, encoder_outputs, each_input_size, i):\n\n        # Compute context vector.\n        if i == 0:\n            context = torch.mean(encoder_outputs, dim=0, keepdim=True)\n            # Compute gru output.\n            gru_input = torch.cat((last_palette, context.squeeze(0)), 1)\n            gru_hidden = self.gru(gru_input, last_decoder_hidden)\n\n            # Generate palette color.\n            #palette = self.out(gru_hidden.squeeze(0))\n            palette = self.out(gru_hidden.squeeze(1))\n            return palette, context.unsqueeze(0), gru_hidden, None\n\n        else:\n            \n            # Calculate alpha (Equation 9 and 10 in paper)\n            attn_weights = self.attn(last_decoder_hidden.squeeze(0), encoder_outputs, each_input_size)\n            \n            # Calculate context vector (Equation 8 in paper)\n            context = torch.bmm(attn_weights, encoder_outputs.transpose(0,1))\n\n            # Compute gru output.\n            gru_input = torch.cat((last_palette, context.squeeze(1)), 1)\n            gru_hidden = self.gru(gru_input, last_decoder_hidden)\n\n            # Generate palette color.\n            #palette = self.out(gru_hidden.squeeze(0))\n            palette = self.out(gru_hidden.squeeze(1))\n            return palette, context.unsqueeze(0), gru_hidden, attn_weights\n\n\nclass Attn(nn.Module):\n    def __init__(self, hidden_size, max_length):\n        super(Attn, self).__init__()\n        self.hidden_size = hidden_size\n        self.softmax = nn.Softmax(dim=0)\n        self.attn_e = nn.Linear(self.hidden_size, self.hidden_size)\n        self.attn_h = nn.Linear(self.hidden_size, self.hidden_size)\n        self.attn_energy = nn.Linear(self.hidden_size, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, hidden, encoder_outputs, each_size):\n        seq_len = encoder_outputs.size(0)\n        batch_size = encoder_outputs.size(1)\n        attn_energies = torch.zeros(seq_len,batch_size,1).to(device)\n\n        for i in range(seq_len):\n            attn_energies[i] = self.score(hidden, encoder_outputs[i])\n\n        attn_energies = self.softmax(attn_energies) # (seq_len, batch_size, 1)\n        return attn_energies.permute(1,2,0)         # (batch_size, 1, seq_len)\n\n    def score(self, hidden, encoder_output):\n        encoder_ = self.attn_e(encoder_output)  # encoder output (batch_size, hidden_size)\n        hidden_ = self.attn_h(hidden)           # hidden (batch_size, hidden_size)\n        energy = self.attn_energy(self.sigmoid(encoder_ + hidden_))\n\n        return energy\n\n\n# class Discriminator(nn.Module):\n#     def __init__(self, color_size=15, hidden_dim=150):\n#         super(Discriminator, self).__init__()\n#         curr_dim = color_size+hidden_dim\n\n#         layers = []\n#         layers.append(nn.Linear(curr_dim, int(curr_dim/2)))\n#         layers.append(nn.ReLU(inplace=True))\n#         layers.append(nn.Linear(int(curr_dim/2), int(curr_dim/4)))\n#         layers.append(nn.ReLU(inplace=True))\n#         layers.append(nn.Linear(int(curr_dim/4), int(curr_dim/8)))\n#         layers.append(nn.ReLU(inplace=True))\n#         layers.append(nn.Linear(int(curr_dim/8), 1)) # 9 -> 1\n#         layers.append(nn.Sigmoid())\n\n#         self.main = nn.Sequential(*layers)\n\n#     def forward(self, color, text):\n#         out = torch.cat([color, text], dim=1) # color: batch x 15, text: batch x 150\n#         out2 = self.main(out)\n#         return out2.squeeze(1)","metadata":{"execution":{"iopub.status.busy":"2022-12-14T10:02:03.030166Z","iopub.execute_input":"2022-12-14T10:02:03.030620Z","iopub.status.idle":"2022-12-14T10:02:03.061517Z","shell.execute_reply.started":"2022-12-14T10:02:03.030573Z","shell.execute_reply":"2022-12-14T10:02:03.060362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PCN","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch import optim\nimport torch.optim.lr_scheduler as scheduler\nimport torch.nn.functional as F\n# import torch.nn.sigmoid as F\n\n\nclass UNetConvBlock1_1(nn.Module):\n    def __init__(self, in_size, out_size, kernel_size=3):\n        super(UNetConvBlock1_1, self).__init__()\n        self.conv = nn.Conv2d(in_size, out_size, kernel_size, padding=1)\n\n    def forward(self, x):\n        out = self.conv(x)\n        return out\n\nclass UNetConvBlock1_2(nn.Module):\n    def __init__(self, in_size, out_size, kernel_size=3, activation=F.relu):\n        super(UNetConvBlock1_2, self).__init__()\n        self.conv2 = nn.Conv2d(in_size, out_size, kernel_size, padding=1)\n        self.activation = activation\n        self.batchnorm = nn.BatchNorm2d(out_size)\n        self.conv3 = nn.Conv2d(out_size, out_size, 1, stride=2, groups=out_size, bias=False)\n\n    def forward(self, x):\n        out = self.activation(x)\n        out = self.activation(self.conv2(out))\n        out = self.batchnorm(out)\n        out = self.conv3(out)\n        return out\n\nclass UNetConvBlock1_2_2(nn.Module):\n    def __init__(self, in_size, out_size, kernel_size=3, activation=F.relu):\n        super(UNetConvBlock1_2_2, self).__init__()\n        self.conv2 = nn.Conv2d(in_size, out_size, kernel_size, padding=1)\n        self.activation = activation\n        self.batchnorm = nn.BatchNorm2d(out_size)\n\n    def forward(self, x):\n        out = self.activation(x)\n        out = self.activation(self.conv2(out))\n        out = self.batchnorm(out)\n        return out\n\nclass UNetConvBlock2(nn.Module):\n    def __init__(self, in_size, out_size, kernel_size=3, activation=F.relu):\n        super(UNetConvBlock2, self).__init__()\n        self.conv = nn.Conv2d(in_size, out_size, kernel_size, padding=1)\n        self.conv2 = nn.Conv2d(out_size, out_size, kernel_size, padding=1)\n        self.activation = activation\n        self.batchnorm = nn.BatchNorm2d(out_size)\n        self.conv3 = nn.Conv2d(out_size, out_size, 1, stride=2, groups=out_size, bias=False)\n\n    def forward(self, x):\n        out = self.activation(self.conv(x))\n        out = self.activation(self.conv2(out))\n        out = self.batchnorm(out)\n        out = self.conv3(out)\n        return out\n\nclass UNetConvBlock2_2(nn.Module):\n    def __init__(self, in_size, out_size, kernel_size=3, activation=F.relu):\n        super(UNetConvBlock2_2, self).__init__()\n        self.conv = nn.Conv2d(in_size, out_size, kernel_size, padding=1)\n        self.conv2 = nn.Conv2d(out_size, out_size, kernel_size, padding=1)\n        self.activation = activation\n        self.batchnorm = nn.BatchNorm2d(out_size)\n\n    def forward(self, x):\n        out = self.activation(self.conv(x))\n        out = self.activation(self.conv2(out))\n        out = self.batchnorm(out)\n        return out\n\nclass UNetConvBlock3(nn.Module):\n    def __init__(self, in_size, out_size, kernel_size=3, activation=F.relu):\n        super(UNetConvBlock3, self).__init__()\n        self.conv = nn.Conv2d(in_size, out_size, kernel_size, padding=1)\n        self.conv2 = nn.Conv2d(out_size, out_size, kernel_size, padding=1)\n        self.conv3 = nn.Conv2d(out_size, out_size, kernel_size, padding=1)\n        self.activation = activation\n        self.batchnorm = nn.BatchNorm2d(out_size)\n        self.conv4 = nn.Conv2d(out_size, out_size, 1, stride=2, groups=out_size, bias=False)\n\n    def forward(self, x):\n        out = self.activation(self.conv(x))\n        out = self.activation(self.conv2(out))\n        out = self.activation(self.conv3(out))\n        out = self.batchnorm(out)\n        out = self.conv4(out)\n        return out\n\nclass UNetConvBlock3_2(nn.Module):\n    def __init__(self, in_size, out_size, kernel_size=3, activation=F.relu):\n        super(UNetConvBlock3_2, self).__init__()\n        self.conv = nn.Conv2d(in_size, out_size, kernel_size, padding=1)\n        self.conv2 = nn.Conv2d(out_size, out_size, kernel_size, padding=1)\n        self.conv3 = nn.Conv2d(out_size, out_size, kernel_size, padding=1)\n        self.activation = activation\n        self.batchnorm = nn.BatchNorm2d(out_size)\n\n    def forward(self, x):\n        out = self.activation(self.conv(x))\n        out = self.activation(self.conv2(out))\n        out = self.activation(self.conv3(out))\n        out = self.batchnorm(out)\n        return out\n\nclass UNetConvBlock4(nn.Module):\n    def __init__(self, in_size, out_size, kernel_size=3, activation=F.relu):\n        super(UNetConvBlock4, self).__init__()\n        self.conv = nn.Conv2d(in_size, out_size, kernel_size, padding=1, dilation=1)\n        self.conv2 = nn.Conv2d(out_size, out_size, kernel_size, padding=1, dilation=1)\n        self.conv3 = nn.Conv2d(out_size, out_size, kernel_size, padding=1, dilation=1)\n        self.activation = activation\n        self.batchnorm = nn.BatchNorm2d(out_size)\n\n    def forward(self, x):\n        out = self.activation(self.conv(x))\n        out = self.activation(self.conv2(out))\n        out = self.activation(self.conv3(out))\n        out = self.batchnorm(out)\n        return out\n\nclass UNetConvBlock5(nn.Module):\n    def __init__(self, in_size, out_size, kernel_size=3, activation=F.relu):\n        super(UNetConvBlock5, self).__init__()\n        self.conv = nn.Conv2d(in_size, out_size, kernel_size, padding=2, dilation=2)\n        self.conv2 = nn.Conv2d(out_size, out_size, kernel_size, padding=2, dilation=2)\n        self.conv3 = nn.Conv2d(out_size, out_size, kernel_size, padding=2, dilation=2)\n        self.activation = activation\n        self.batchnorm = nn.BatchNorm2d(out_size)\n\n    def forward(self, x):\n        out = self.activation(self.conv(x))\n        out = self.activation(self.conv2(out))\n        out = self.activation(self.conv3(out))\n        out = self.batchnorm(out)\n        return out\n\nclass UNetConvBlock6(nn.Module):\n    def __init__(self, in_size, out_size, kernel_size=3, activation=F.relu):\n        super(UNetConvBlock6, self).__init__()\n        self.conv = nn.Conv2d(in_size, out_size, kernel_size, padding=2, dilation=2)\n        self.conv2 = nn.Conv2d(out_size, out_size, kernel_size, padding=2, dilation=2)\n        self.conv3 = nn.Conv2d(out_size, out_size, kernel_size, padding=2, dilation=2)\n        self.activation = activation\n        self.batchnorm = nn.BatchNorm2d(out_size)\n\n    def forward(self, x):\n        out = self.activation(self.conv(x))\n        out = self.activation(self.conv2(out))\n        out = self.activation(self.conv3(out))\n        out = self.batchnorm(out)\n        return out\n\nclass UNetConvBlock7(nn.Module):\n    def __init__(self, in_size, out_size, kernel_size=3, activation=F.relu):\n        super(UNetConvBlock7, self).__init__()\n        self.conv = nn.Conv2d(in_size, out_size, kernel_size, padding=1, dilation=1)\n        self.conv2 = nn.Conv2d(out_size, out_size, kernel_size, padding=1, dilation=1)\n        self.conv3 = nn.Conv2d(out_size, out_size, kernel_size, padding=1, dilation=1)\n        self.activation = activation\n        self.batchnorm = nn.BatchNorm2d(out_size)\n\n    def forward(self, x):\n        out = self.activation(self.conv(x))\n        out = self.activation(self.conv2(out))\n        out = self.activation(self.conv3(out))\n        out = self.batchnorm(out)\n        return out\n\nclass UNetConvBlock8(nn.Module):\n    def __init__(self, in_size, out_size, kernel_size=3, activation=F.relu, space_dropout=False):\n        super(UNetConvBlock8, self).__init__()\n        self.up = nn.ConvTranspose2d(in_size, out_size, 4, stride=2, padding=1, dilation=1)\n        self.bridge = nn.Conv2d(256, 256, kernel_size, padding=1)\n\n        self.conv = nn.Conv2d(out_size, out_size, kernel_size, padding=1, dilation=1)\n        self.conv2 = nn.Conv2d(out_size, out_size, kernel_size, padding=1, dilation=1)\n        self.activation = activation\n        self.batchnorm = nn.BatchNorm2d(out_size)\n\n    def forward(self, x, bridge):\n        up = self.up(x)\n        out = self.activation(self.bridge(bridge) + up)\n        out = self.activation(self.conv(out))\n        out = self.activation(self.conv2(out))\n        out = self.batchnorm(out)\n        return out\n\nclass UNetConvBlock9(nn.Module):\n    def __init__(self, in_size, out_size, kernel_size=3, activation=F.relu, space_dropout=False):\n        super(UNetConvBlock9, self).__init__()\n        self.up = nn.ConvTranspose2d(in_size, out_size, 4, stride=2, padding=1, dilation=1)\n        self.bridge = nn.Conv2d(128, 128, kernel_size, padding=1)\n        self.conv = nn.Conv2d(out_size, out_size, kernel_size, padding=1, dilation=1)\n        self.activation = activation\n        self.batchnorm = nn.BatchNorm2d(out_size)\n\n    def forward(self, x, bridge):\n        up = self.up(x)\n        out = self.activation(self.bridge(bridge) + up)\n        out = self.activation(self.conv(out))\n        out = self.batchnorm(out)\n\n        return out\n\nclass UNetConvBlock10(nn.Module):\n    def __init__(self, in_size, out_size, kernel_size=3, activation=F.relu, space_dropout=False):\n        super(UNetConvBlock10, self).__init__()\n        self.up = nn.ConvTranspose2d(in_size, out_size, 4, stride=2, padding=1, dilation=1)\n        self.bridge = nn.Conv2d(64, 128, kernel_size, padding=1)\n        self.conv = nn.Conv2d(out_size, out_size, kernel_size, padding=1, dilation=1)\n        self.activation = activation\n        self.activation2 = nn.LeakyReLU(negative_slope=0.02)\n\n    def forward(self, x, bridge):\n        up = self.up(x)\n        out = self.activation(self.bridge(bridge) + up)\n        out = self.activation2(self.conv(out))\n        return out\n\nclass prediction(nn.Module):\n    def __init__(self, in_size, out_size, kernel_size=1, activation=F.sigmoid, space_dropout=False):\n        super(prediction, self).__init__()\n        self.conv = nn.Conv2d(in_size, out_size, kernel_size, dilation=1)\n        self.activation = activation\n\n    def forward(self, x):\n        out = self.activation(self.conv(x))\n        return out\n\nclass convrelu(nn.Module):\n\n    def __init__(self, in_size, out_size, kernel_size=1, activation=F.relu, space_dropout=False):\n        super(convrelu, self).__init__()\n        self.conv = nn.Conv2d(in_size, out_size, kernel_size, padding=0)\n        self.activation = activation\n\n    def forward(self, x):\n        out = self.activation(self.conv(x))\n        return out\n\nclass global_network(nn.Module):\n    def __init__(self, image_size, add_L):\n        super(global_network, self).__init__()\n        if add_L:\n            self.oneD = convrelu(16, 128)\n        else:\n            self.oneD = convrelu(11, 128)\n        self.twoD = convrelu(128, 256)\n        self.threeD = convrelu(256, 512)\n        self.fourD = convrelu(512, 512)\n        self.image_size = image_size\n\n    def forward(self, x, dim):\n        n = 2\n        out = self.oneD(x)\n        \n        if dim >= 256:\n            n = 4\n            out = self.twoD(out)\n        if dim == 512:\n            n = 8\n            out = self.threeD(out)\n            out = self.fourD(out)\n\n        out = out.repeat(1,1, int(self.image_size/n), int(self.image_size/n))\n        return out\n\n\nclass UNet(nn.Module):\n    def __init__(self, imsize, multi_injection, add_L):\n        super(UNet, self).__init__()\n        self.imsize = imsize\n        self.multi_injection = multi_injection\n        self.globalnet512 = global_network(self.imsize, add_L)\n\n        if multi_injection:\n            self.globalnet256 = global_network(self.imsize, add_L)\n            self.globalnet128 = global_network(self.imsize, add_L)\n\n        self.convlayer1_1 = UNetConvBlock1_1(1, 64)\n        self.convlayer1_2 = UNetConvBlock1_2(64, 64)\n        self.convlayer1_2_2 = UNetConvBlock1_2_2(64, 64)\n        self.convlayer2 = UNetConvBlock2(64, 128)\n        self.convlayer2_2 = UNetConvBlock2_2(64, 128)\n        self.convlayer3 = UNetConvBlock3(128, 256)\n        self.convlayer3_2 = UNetConvBlock3_2(128, 256)\n        self.convlayer4 = UNetConvBlock4(256, 512)\n        self.convlayer5 = UNetConvBlock5(512, 512)\n        self.convlayer6 = UNetConvBlock6(512, 512)\n        self.convlayer7 = UNetConvBlock7(512, 512)\n        self.convlayer8 = UNetConvBlock8(512, 256)\n        self.convlayer9 = UNetConvBlock9(256, 128)\n        self.convlayer10 = UNetConvBlock10(128, 128)\n\n        self.prediction = prediction(128, 2)\n\n    def forward(self, x, side_input):\n        layer1_1 = self.convlayer1_1(x)\n        layer1_2 = self.convlayer1_2(layer1_1)\n        layer1_2_2 = self.convlayer1_2_2(layer1_1)\n        layer2 = self.convlayer2(layer1_2)\n        layer2_2 = self.convlayer2_2(layer1_2)\n        layer3 = self.convlayer3(layer2)\n        layer3_2 = self.convlayer3_2(layer2)\n        layer4 = self.convlayer4(layer3)\n\n        global_net512 = self.globalnet512(side_input, 512)\n        layer4 = layer4 + global_net512\n        layer5 = self.convlayer5(layer4)\n        layer6 = self.convlayer6(layer5)\n        layer7 = self.convlayer7(layer6)\n\n        layer8 = self.convlayer8(layer7, layer3_2)\n        if self.multi_injection:\n            global_net256 = self.globalnet256(side_input, 256)\n            layer8 = layer8 + global_net256\n        \n        layer9 = self.convlayer9(layer8, layer2_2)\n        if self.multi_injection:\n            global_net128 = self.globalnet128(side_input, 128)\n            layer9 = layer9 + global_net128\n        \n        layer10 = self.convlayer10(layer9, layer1_2_2)\n\n        prediction = self.prediction(layer10)\n\n        return prediction\n\n\nclass Discriminator(nn.Module):\n\n    def __init__(self, add_L, imsize, conv_dim=64, repeat_num=5):\n        super(Discriminator, self).__init__()\n\n        input_dim = 2 + 10\n        if add_L:\n            input_dim = 3 + 15\n\n        layers = []\n        layers.append(nn.Conv2d(input_dim, conv_dim, kernel_size=4, stride=2, padding=1))\n        layers.append(nn.LeakyReLU(0.01, inplace=True))\n\n        curr_dim = conv_dim\n        for i in range(1, repeat_num):\n            layers.append(nn.Conv2d(curr_dim, curr_dim*2, kernel_size=4, stride=2, padding=1))\n            layers.append(nn.LeakyReLU(0.01, inplace=True))\n            curr_dim = curr_dim * 2\n\n        k_size = int(imsize / np.power(2, repeat_num))\n        self.main = nn.Sequential(*layers)\n        self.conv1 = nn.Conv2d(curr_dim, curr_dim, kernel_size=3, stride=1, padding=1, bias=False)\n\n        self.fc = nn.Sequential(\n            nn.BatchNorm1d(k_size*k_size*curr_dim),\n            nn.Linear(k_size*k_size*curr_dim, 1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        batch_size = x.size(0)\n        h = self.main(x)\n        out = self.conv1(h)\n        out = out.view(batch_size, -1)\n        out = self.fc(out)\n        return out\n\n\ndef init_models(batch_size, imsize, dropout_ep, learning_rate, multi_injection, \n                add_L, weight_decay=1e-7):\n\n    G = UNet(imsize, multi_injection, add_L).cuda()\n    print('# parameters of Generator : ',num_param(G))\n    D = Discriminator(add_L, imsize).cuda()\n    print('# parameters of Discriminator : ',num_param(D))\n    G_optimizer = optim.Adam(G.parameters(), lr=learning_rate, weight_decay=weight_decay)\n    D_optimizer = optim.Adam(D.parameters(), lr=learning_rate, weight_decay=weight_decay)\n\n    G_scheduler = scheduler.ReduceLROnPlateau(G_optimizer, 'min', patience=5, factor = 0.1)\n    D_scheduler = scheduler.ReduceLROnPlateau(D_optimizer, 'min', patience=5, factor = 0.1)\n\n    return (G, D, G_optimizer, D_optimizer, G_scheduler, D_scheduler)","metadata":{"execution":{"iopub.status.busy":"2022-12-14T10:02:03.063701Z","iopub.execute_input":"2022-12-14T10:02:03.064157Z","iopub.status.idle":"2022-12-14T10:02:03.151188Z","shell.execute_reply.started":"2022-12-14T10:02:03.064119Z","shell.execute_reply":"2022-12-14T10:02:03.149838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load model\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Solver","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport os\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.switch_backend('agg')\nfrom skimage.color import lab2rgb\n\n\nclass Solver(object):\n    def __init__(self, args):\n        self.args = args\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        # Build the model.\n        self.build_model(args.mode)\n\n    def prepare_dict(self):\n        input_dict = Dictionary()\n        src_path = os.path.join('/kaggle/input/data-text2color/data/hexcolor_vf/all_names.pkl')\n        with open(src_path, 'rb') as f:\n            text_data = pickle.load(f)\n            f.close()\n\n        print(\"Loading %s palette names...\" % len(text_data))\n        print(\"Making text dictionary...\")\n\n        for i in range(len(text_data)):\n            input_dict.index_elements(text_data[i])\n        return input_dict\n\n    def prepare_data(self, images, palettes, always_give_global_hint, add_L):\n        batch = images.size(0)\n        imsize = images.size(3)\n\n        inputs, labels = process_image(images, batch, imsize)\n        if add_L:\n            for_global = process_palette_lab(palettes, batch)\n            global_hint = process_global_lab(for_global, batch, always_give_global_hint)\n        else:\n            for_global = process_palette_ab(palettes, batch)\n            global_hint = process_global_ab(for_global, batch, always_give_global_hint)\n\n        inputs = inputs.to(self.device)\n        labels = labels.to(self.device)\n        global_hint = (global_hint).expand(-1, -1, imsize, imsize).to(self.device)\n        return inputs, labels, global_hint\n\n\n    def build_model(self, mode):\n        # Data loader.\n        self.input_dict = self.prepare_dict()\n\n        # Load pre-trained GloVe embeddings.\n        emb_file = os.path.join('/kaggle/input/data-text2color/data', 'Color-Hex-vf.pth')\n        if os.path.isfile(emb_file):\n            W_emb = torch.load(emb_file)\n        else:\n            W_emb = load_pretrained_embedding(self.input_dict.word2index,\n                                              '/kaggle/input/glove840b300dtxt/glove.840B.300d.txt',\n                                              300)\n            W_emb = torch.from_numpy(W_emb)\n            torch.save(W_emb, emb_file)\n        W_emb = W_emb.to(self.device)\n\n        # Data loader.\n        self.test_loader, self.imsize = test_loader(self.args.dataset, self.args.batch_size, self.input_dict)\n\n        # Load the trained generators.\n        self.encoder = EncoderRNN(self.input_dict.n_words, self.args.hidden_size,\n                                      self.args.n_layers, self.args.dropout_p, W_emb).to(self.device)\n        self.G_TPN = AttnDecoderRNN(self.input_dict, self.args.hidden_size,\n                                    self.args.n_layers, self.args.dropout_p).to(self.device)\n        self.G_PCN = UNet(self.imsize, self.args.multi_injection, self.args.add_L).to(self.device)\n\n\n    def load_model(self, mode, resume_epoch):\n        print('Loading the trained model from epoch {}...'.format(resume_epoch))\n        encoder_path = os.path.join(self.args.text2pal_dir, '{}_G_encoder.ckpt'.format(resume_epoch))\n        G_TPN_path = os.path.join(self.args.text2pal_dir, '{}_G_decoder.ckpt'.format(resume_epoch))\n        G_PCN_path = os.path.join(self.args.pal2color_dir, 'model.ckpt')\n        self.encoder.load_state_dict(torch.load(encoder_path, map_location=lambda storage, loc: storage), strict=False)\n        self.G_TPN.load_state_dict(torch.load(G_TPN_path, map_location=lambda storage, loc: storage), strict=False)\n        self.G_PCN.load_state_dict(torch.load(G_PCN_path, map_location=lambda storage, loc: storage), strict=False)\n\n\n    \n    def test_text2colors(self):\n        # Load model.\n        if self.args.resume_epoch:\n            self.load_model(self.args.mode, self.args.resume_epoch)\n\n        print('Start testing...')\n        for batch_idx, (txt_embeddings, real_palettes, images) in enumerate(self.test_loader):\n            if txt_embeddings.size(0) != self.args.batch_size:\n                break\n\n            # Compute text input size (without zero padding).\n            batch_size = txt_embeddings.size(0)\n            nonzero_indices = list(torch.nonzero(txt_embeddings)[:, 0])\n            each_input_size = [nonzero_indices.count(j) for j in range(batch_size)]\n\n            # Prepare test data.\n            txt_embeddings = txt_embeddings.to(self.device)\n            real_palettes = real_palettes.to(self.device).float()\n\n            for num_gen in range(5):\n                # Prepare input and output variables.\n                palette = torch.FloatTensor(batch_size, 3).zero_().to(self.device)\n                fake_palettes = torch.FloatTensor(batch_size, 15).zero_().to(self.device)\n                # ============================== Text-to-Palette ==============================#\n                # Condition for the generator.\n                encoder_hidden = self.encoder.init_hidden(batch_size).to(self.device)\n                encoder_outputs, decoder_hidden, mu, logvar = self.encoder(txt_embeddings, encoder_hidden)\n\n                # Generate color palette.\n                for i in range(5):\n                    palette, decoder_context, decoder_hidden, _ = self.G_TPN(palette,\n                                                                             decoder_hidden.squeeze(0),\n                                                                             encoder_outputs,\n                                                                             each_input_size,\n                                                                             i)\n                    fake_palettes[:, 3 * i:3 * (i+1)] = palette\n\n                # ========================= Palette-to-Colorization ============================#\n                # Prepare testing data.\n                inputs, labels = process_image(images, batch_size, self.imsize)\n                inputs, labels = inputs.to(self.device), labels.to(self.device)\n\n                fake_palettes_ = fake_palettes.view(-1, 5, 3).cpu().data.numpy()\n                for_global = process_palette_lab(fake_palettes_, batch_size)\n                side_inputs = process_global_sampling_lab(for_global, batch_size, self.imsize, 0.03, 0.13)\n\n                fake_images = self.G_PCN(inputs, side_inputs)\n                # ================================ Save Results ================================#\n                colored_img = torch.cat([inputs, fake_images], 1).data.cpu().numpy().transpose((0, 2, 3, 1))\n                gt_img = images.cpu().numpy().transpose((0, 2, 3, 1))\n\n                for x in range(batch_size):\n                    # Input text.\n                    input_text = ''\n                    for idx in txt_embeddings[x]:\n                        if idx.item() == 0: break\n                        input_text += self.input_dict.index2word[idx.item()] + \" \"\n\n                    ## Save palette generation results.\n                    fig1, axs1 = plt.subplots(nrows=2, ncols=5)\n                    axs1[0][0].set_title(input_text + 'fake {}'.format(num_gen+1))\n                    for k in range(5):\n                        lab = np.array([fake_palettes.data[x][3*k],\n                                        fake_palettes.data[x][3*k+1],\n                                        fake_palettes.data[x][3*k+2]], dtype='float64')\n                        rgb = lab2rgb_1d(lab)\n                        axs1[0][k].imshow([[rgb]])\n                        axs1[0][k].axis('off')\n                    axs1[1][0].set_title(input_text + 'real')\n                    for k in range(5):\n                        lab = np.array([real_palettes.data[x][3*k],\n                                        real_palettes.data[x][3*k+1],\n                                        real_palettes.data[x][3*k+2]], dtype='float64')\n                        rgb = lab2rgb_1d(lab)\n                        axs1[1][k].imshow([[rgb]])\n                        axs1[1][k].axis('off')\n\n                    fig1.savefig(os.path.join(self.args.test_sample_dir, self.args.mode,\n                                              '{}_palette{}.jpg'.format(self.args.batch_size*batch_idx+x+1,num_gen+1)))\n\n                    ## Save colorization results.\n                    fig2, axs2 = plt.subplots(ncols=2)\n\n                    # Make images back to RGB.\n                    colored_img[k][:, :, 0] = colored_img[k][:, :, 0] * 100\n                    colored_img[k][:, :, 1] = (colored_img[k][:, :, 1] * 185) - 88\n                    colored_img[k][:, :, 2] = (colored_img[k][:, :, 2] * 212) - 127\n                    colored_img[k] = lab2rgb(colored_img[k].astype(np.uint8), illuminant='D50')\n\n                    gt_img[k][:, :, 0] = gt_img[k][:, :, 0] * 100\n                    gt_img[k][:, :, 1] = (gt_img[k][:, :, 1] * 185) - 88\n                    gt_img[k][:, :, 2] = (gt_img[k][:, :, 2] * 212) - 127\n                    gt_img[k] = lab2rgb(gt_img[k].astype(np.uint8), illuminant='D50')\n\n                    axs2[0].set_title(input_text + '/ Prediction')\n                    axs2[0].imshow(colored_img[x])\n                    axs2[0].axis('off')\n\n                    axs2[1].set_title(input_text + '/ Ground Truth')\n                    axs2[1].imshow(gt_img[x])\n                    axs2[1].axis('off')\n\n                    fig2.savefig(os.path.join(self.args.test_sample_dir, self.args.mode,\n                                              '{}_color{}.jpg'.format(self.args.batch_size*batch_idx+x+1, num_gen+1)))\n                    print('Saved data [{}], input text [{}], test sample [{}]'.format(\n                          self.args.batch_size*batch_idx+x+1, input_text, num_gen+1))","metadata":{"execution":{"iopub.status.busy":"2022-12-14T10:02:03.218883Z","iopub.execute_input":"2022-12-14T10:02:03.219321Z","iopub.status.idle":"2022-12-14T10:02:03.268192Z","shell.execute_reply.started":"2022-12-14T10:02:03.219284Z","shell.execute_reply":"2022-12-14T10:02:03.266621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load model checkpoint","metadata":{}},{"cell_type":"code","source":"# model = torch.load('/kaggle/input/model-pcn/model.ckpt')","metadata":{"execution":{"iopub.status.busy":"2022-12-14T10:02:03.270373Z","iopub.execute_input":"2022-12-14T10:02:03.270784Z","iopub.status.idle":"2022-12-14T10:02:03.283718Z","shell.execute_reply.started":"2022-12-14T10:02:03.270747Z","shell.execute_reply":"2022-12-14T10:02:03.282508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TEST","metadata":{}},{"cell_type":"code","source":"from __future__ import division\nimport os\nimport argparse\n\nimport sys\nsys.argv=['']\ndel sys\n\n\ndef main(args):\n\n    # Create directory if it doesn't exist.\n    if not os.path.exists(args.text2pal_dir):\n        os.makedirs(args.text2pal_dir)\n    if not os.path.exists(args.pal2color_dir):\n        os.makedirs(args.pal2color_dir)\n    if not os.path.exists(args.train_sample_dir):\n        os.makedirs(args.train_sample_dir)\n    if not os.path.exists(os.path.join(args.test_sample_dir, args.mode)):\n        os.makedirs(os.path.join(args.test_sample_dir, args.mode))\n\n    # Solver for training and testing Text2Colors.\n    solver = Solver(args)\n\n    # Train or test.\n    if args.mode == 'train_TPN':\n        solver.train_TPN()\n\n    elif args.mode == 'train_PCN':\n        solver.train_PCN()\n\n    elif args.mode == 'test_TPN':\n        solver.test_TPN()\n\n    elif args.mode == 'test_text2colors':\n        solver.test_text2colors()\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n\n    # Model configuration.\n    # text2pal\n    parser.add_argument('--hidden_size', type=int, default=150)\n    parser.add_argument('--n_layers', type=int, default=1)\n    # pal2color\n    parser.add_argument('--always_give_global_hint', type=int, default=1)\n    parser.add_argument('--add_L', type=int, default=1)\n    parser.add_argument('--multi_injection', type=int, default=1)\n\n    # Training and testing configuration.\n    parser.add_argument('--mode', type=str, default='test_text2colors',\n                        choices=['train_TPN', 'train_PCN', 'test_TPN', 'test_text2colors'])\n    parser.add_argument('--dataset', type=str, default='bird256', choices=['imagenet', 'bird256'])\n    parser.add_argument('--lr', type=float, default=5e-4, help='initial learning rate')\n    parser.add_argument('--num_epochs', type=int, default=1000, help='number of epochs for training')\n    parser.add_argument('--resume_epoch', type=int, default=950, help='resume training from this epoch')\n    parser.add_argument('--batch_size', type=int, default=8, help='batch size for training')\n    parser.add_argument('--dropout_p', type=float, default=0.2)\n    parser.add_argument('--weight_decay', type=float, default=5e-5)\n    parser.add_argument('--beta1', type=float, default=0.5)\n    parser.add_argument('--beta2', type=float, default=0.99)\n    parser.add_argument('--lambda_sL1', type=float, default=100.0, help='weight for L1 loss')\n    parser.add_argument('--lambda_KL', type=float, default=0.5, help='weight for KL loss')\n    parser.add_argument('--lambda_GAN', type=float, default=0.1)\n\n    # Directories.\n    parser.add_argument('--text2pal_dir', type=str, default='/kaggle/input/checkpoint')\n    parser.add_argument('--pal2color_dir', type=str, default='/kaggle/input/model-pcn')\n    parser.add_argument('--train_sample_dir', type=str, default='./samples/train')\n    parser.add_argument('--test_sample_dir', type=str, default='./samples/test')\n\n    # Step size.\n    parser.add_argument('--log_interval', type=int, default=1,\n                        help='how many steps to wait before logging training status')\n    parser.add_argument('--sample_interval', type=int, default=20,\n                        help='how many steps to wait before saving the training output')\n    parser.add_argument('--save_interval', type=int, default=50,\n                        help='how many steps to wait before saving the trained models')\n    args = parser.parse_args()\n    print(args)\n    main(args)","metadata":{"execution":{"iopub.status.busy":"2022-12-14T10:02:03.285603Z","iopub.execute_input":"2022-12-14T10:02:03.285992Z"},"trusted":true},"execution_count":null,"outputs":[]}]}